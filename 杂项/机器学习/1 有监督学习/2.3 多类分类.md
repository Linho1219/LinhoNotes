# 2.3 多类分类

在二元分类中，目标变量 $y$ 的可能取值只有 $\set{0, 1}$；现在我们将二元分类进行拓展，让 $y$ 取 $\set{1, 2, \cdots, k}$：即更多的类。我们该如何建模呢？

在这种情况下，$p(y \mid x; \theta)$ 是多项分布。多项分布包含了 $k$ 个参数 $\phi_1, \phi_2, \cdots, \phi_k$，它们表示了出现各个结果的概率，并满足 $\sum_{i = 1}^k \phi_k = 1$。我们将会构造一个模型，使得输出的 $\phi_1, \cdots, \phi_k$ 满足上述条件。

我们引入 $k$ 组参数 $\theta_1, \cdots, \theta_k$，每一个都是 $\mathbb R^d$ 中的向量。依照我们先前的经验，我们会使用 $\theta_1^{\text T} x, \cdots, \theta_k^{\text T} x$ 来表示 $\phi_1, \cdots, \phi_k$，但这种方法有两种问题：一是 $\theta_j^{\text T} x$ 不一定落在区间 $[0, 1]$ 内部；二是 $\sum_{j = 1}^{n}\theta_j^{\text T} x$ 不一定为 $1$。因此，我们不能使用这种简单的方法。一种解决思路是使用 $\text{softmax}$ 函数将 $(\theta_1^{\text T} x, \cdots, \theta_k^{\text T}x)$ 转化为概率向量，使得每一个分量都非负且总和为 $1$。

定义函数 $\text {softmax}: \mathbb R^k \to \mathbb R^k$ 如下
$$
\text{softmax} (t_1, \cdots, t_k) = \begin{bmatrix}
\frac{\exp (t_1)}{\sum_{j = 1}^{k} \exp(t_j)} \\
\vdots \\
\frac{\exp (t_k)}{\sum_{j = 1}^{k} \exp(t_j)} \\
\end{bmatrix}.
$$
其中，输入项向量 $t$ 常常被称作逻辑特 (logits)。

下面，我们令 $(t_1, \cdots, t_k) = (\theta_1^{\text T} x, \cdots, \theta_k^{\text T} x)$，这样我们就能够利用 $\text{softmax}$ 函数，将线性向量转变为合法的概率向量。我们不妨将输出的概率向量作为条件概率 $P(y = 1 \mid x; \theta), \cdots, P(y = k \mid x; \theta)$，然后我们就得到了下面的概率模型
$$
\begin{bmatrix}
P(y = 1 \mid x; \theta) \\
\vdots \\
P(y = k \mid x; \theta) \\
\end{bmatrix} = \text{softmax} (t_1, \cdots, t_k) = 
\begin{bmatrix}
\frac{\exp (\theta_1^{\text T} x)}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x)} \\
\vdots \\
\frac{\exp (\theta_k^{\text T} x)}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x)} \\
\end{bmatrix}.
$$
为了记号的简洁，我们令 $\phi_i = \frac{\exp (\theta_i^{\text T} x)}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x)}$，于是有
$$
P(y = i \mid x; \theta) = \phi_i.
$$
接下来我们对单个训练样本 $(x, y)$ 计算负对数似然函数：
$$
- \log p(y \mid x, \theta) = - \log \left(
\frac{\exp (t_y)}{\sum_{j = 1}^{k} \exp(t_j)}
\right) = - \log
\left(
\frac{\exp (\theta_y^{\text T} x)}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x)}
\right).
$$
损失函数，即整个训练集的负对数似然函数为
$$
l(\theta) = \sum_{i = 1}^{n} - \log
\left(
\frac{\exp (\theta_{y^{(i)}}^{\text T} x^{(i)})}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x^{(i)})}
\right).
$$
我们定义交叉熵损失函数 $l_{\text{ce}}: \mathbb R^k \times \set{1, \cdots, k} \to \mathbb R_{\ge 0}$ 为
$$
l_{\text {ce}} ((t_1, \cdots, t_k), y) = - \log
\left(
\frac{\exp (\theta_y^{\text T} x)}{\sum_{j = 1}^{k} \exp(\theta_j^{\text T} x)}
\right).
$$
使用这个记号，我们可以把 $l(\theta)$ 改写为
$$
l(\theta) = \sum_{i = 1}^{n} l_{\text{ce}} ((\theta_1^{\text T} x^{(i)}, \cdots, \theta_k^{\text T} x^{(i)}), y^{(i)}).
$$
交叉熵损失函数的梯度表达式极其简洁。如果令 $t = (t_1, \cdots, t_k)$，结合 $\phi_i = \frac{\exp(t_i)}{\sum_{j = 1}^{k} \exp(t_j)}$，可推导出
$$
\frac{\partial}{\partial t_i} l_{\text{ce}}(t, y) = \phi_i - I\set{y = i},
$$
其中 $I\set{\cdot}$ 为指示函数，即
$$
I \set{y = i} = \begin{cases}
1 & \text{if } y = i, \\
0 & \text{otherwise}. \\
\end{cases}
$$
若将上面的梯度写成向量形式，则有
$$
\frac{\partial}{\partial t} l_{\text{ce}}(t, y) = \phi - e_y,
$$
其中 $e_y \in \mathbb R^k$ 是第 $y$ 个自然基向量。由链式法则，我们有
$$
\frac{\partial}{\partial \theta_i} l_{\text{ce}}((\theta_1^{\text T} x, \cdots, \theta_k^{\text T} x), y) = \sum_{j = 1}^{n} (\phi_i^{(j)} - I\set{y^{(i)} = i}) \cdot x^{(j)},
$$
其中 $\phi_i^{(j)} = \frac{\exp(\theta_i^{\text T} x^{(j)})}{\sum_{s = 1}^{k} \exp(\theta_s^{\text T} x^{(j)})}$ 是模型预测 $x^{(i)}$ 样本的输出为 $i$ 的概率。

在计算出上面梯度的基础上，我们可以利用梯度下降法，使得损失函数 $l(\theta)$ 最小化。













