# 1.2 概率理解

在本节，我们主要想解决的问题是为什么使用线性回归，以及为什么定义一个最小二乘代价函数 $J$ 这两个问题。我们主要的探讨思路是给出一系列的概率假设，然后试图推导出这种做法的合理性。

首先，假设目标变量和输入特征之间的关系满足下式：
$$
y^{(i)} = \theta^{\text T} x + \varepsilon^{(i)},
$$
其中 $\varepsilon^{(i)}$ 是误差项，记录了不在建模中的影响因素，或者随机的噪声。我们进一步假设它是独立同分布的高斯噪声：$\varepsilon^{(i)} \stackrel{i.i.d.}{\sim} \mathcal N(0, \sigma^2)$. 由此，我们可以写出其概率密度函数：
$$
f_{\varepsilon^{(i)}}(\varepsilon) = \frac{1}{\sqrt{2 \pi} \sigma} \exp (- \frac{\varepsilon^2}{2 \sigma^2}).
$$
亦即
$$
f(y^{(i)} \mid x^{(i)}; \theta) = \frac{1}{\sqrt {2 \pi} \sigma} \exp (-\frac{(y^{(i)} - \theta^{\text T} x^{(i)})^2}{2 \sigma^2}).
$$

> [!warning]
>
> - 记号 $f(y^{(i)} \mid x^{(i)}; \theta)$ 表示 $y^{(i)}$ 在已知 $x^{(i)}$ 下的条件分布，其中 $\theta$ 为参数，$x^{(i)}$ 和 $\theta$ 中间用分号隔开；
> - 记号 $f(y^{(i)} \mid x^{(i)}, \theta)$ 表示 $y^{(i)}$ 在已知 $x^{(i)}$ 和 $\theta$ 下的条件分布，$x^{(i)}$ 和 $\theta$ 中间用逗号隔开。

上式表示的是单个样本的条件分布，所有样本汇聚在一起的分布是：
$$
f(\vec y \mid X; \theta) = \frac{1}{\sqrt {2 \pi} \sigma} \exp (- \frac{1}{2 \sigma^2} (\vec y - X \theta)^{\text T} \Sigma (\vec y - X \theta)),
$$
其中
$$
\Sigma = \begin{bmatrix}
\sigma^2 \\
& \sigma^2 \\
&& \ddots \\
&&& \sigma^2
\end{bmatrix}.
$$
上面这个函数一般被看作是关于 $\vec y$ 的函数，而我们主要想研究的是 $\theta$ 的行为对分布有着怎样的影响，因此我们需要引入似然函数：
$$
L(\theta) = L(\theta; X, \vec y) = f(\vec y \mid X; \theta).
$$
由 $\varepsilon^{(i)}$ 彼此独立，根据相互独立的定义，上式可改写为
$$
\begin{align}
L(\theta) &= \prod_{i = 1}^{n} f(y^{(i)} \mid x^{(i)}; \theta) \\
&= \prod_{i = 1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp (- \frac{y^{(i)} - \theta^{\text T}x^{(i)}}{2 \sigma^2}).
\end{align}
$$
最大似然估计告诉我们，我们需要找到一个 $\theta$，使得 $L(\theta)$ 取得最大值。考虑到连乘的形式确实不利于求导，我们先取对数，变连乘为累加，再考虑求出 $\theta$：
$$
\begin{align}
l(\theta) &= \log L(\theta) \\
&= \log \prod_{i = 1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp (- \frac{y^{(i)} - \theta^{\text T}x^{(i)}}{2 \sigma^2}) \\
&= \sum_{i = 1}^{n} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp (- \frac{y^{(i)} - \theta^{\text T}x^{(i)}}{2 \sigma^2}) \\
&= -n \log \sqrt{2 \pi} \sigma - \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum_{i = 1}^{n} (y^{(i)} - \theta^{\text T} x^{(i)})^2.
\end{align}
$$
因此，求出使得 $L(\theta)$ 最大的 $\theta$ 等价于求出使得 $J(\theta) = \frac{1}{2} \sum_{i = 1}^{n} (y^{(i)} - \theta^{\text T} x^{(i)})^2$ 最小的 $\theta$. 这也就是我们的代价函数。

说明：我们刚刚基于一系列的概率假设论证了线性回归方法的合理性。但这些概率假设对于说明线性回归的合理性并不是必要的。事实上，存在其他的假设方式，来说明线性回归算法是一个自然的、合理的算法。

注意到我们刚刚求出 $\theta$ 的方法并不依赖于 $\sigma$ 的具体取值。这一点在之后的讨论中也会涉及。