# 2.1 逻辑回归

现在我们来讨论分类问题。在导言中我们提到，目标变量为连续型随机变量的学习问题称为回归问题，目标变量为离散型随机变量的问题称为分类问题。为推导之便利，我们在此只研究目标变量取值为 $\set{0, 1}$ 的情况，即**二元分类**问题。例如，假设我们想要将我们的邮件分为垃圾邮件和非垃圾邮件，设其分别对应 $y = 0$ 和 $y = 1$；$x_j$ 为邮件的输入特征。现在我们就要建立一个假设函数，使其能够根据输入特征预测邮件是否为垃圾邮件。在这里，$0$ 又称为**负类**，$1$ 又称为**正类**，它们有时又写作 $-$ 和 $+$ 。对于给定的 $x^{(i)}$，其对应的 $y^{(i)}$ 又称为训练样本的**标签**。

## 逻辑回归

我们大可以忽略 $y$ 是离散型随机变量这一事实，而用线性回归的知识对其加以解决；但线性回归在应对此类问题上所能发挥的作用极其有限：显然，线性回归的假设函数的值域为 $\mathbb R$，但 $y$ 只能取 $\set{0, 1}$，显然这个假设函数是不合理的。

我们需要对假设函数的值域进行修正。定义
$$
h_{\theta}(x) = g(\theta^{\text T} x) = \frac{1}{1 + e^{-\theta^{\text T} x}},
$$
其中
$$
g(t) = \frac{1}{1 + e^{-t}}
$$
称为逻辑函数或 S-型函数。

可以注意到，$\lim\limits_{t \to - \infty} g(t) = 0, \lim\limits_{t \to + \infty} g(t) = 1$，且 $g(t)$ 在 $\mathbb R$ 上单调递增，这就修正了值域上的问题。

像之前一样，我们设 $x_0 = 1$，则有 $\theta_0 + \sum\limits_{j = 0}^{d} \theta_j x_j = \theta^{\text T} x$.

下面我们研究逻辑函数的一些有趣的性质：导数的特征。
$$
\begin{align}
g'(t) &= \frac{1}{(1 + e^{-t})^2} (e^{-t}) \\
&= \frac{1}{(1 + e^{-t})^2} \left( 1 - \frac{1}{1 + e^{-t}}\right) (1 + e^{-t}) \\
&= g(t) (1 - g(t)).
\end{align}
$$

接下来，我们将利用这一性质，导出随即逻辑回归的算法。

首先，仿照线性回归的概率解释，我们先假设：
$$
\begin{align}
& p(y = 1 \mid x; \theta) = h_{\theta}(x), \\
& p(y = 0 \mid x; \theta) = 1 - h_{\theta}(x). \\
\end{align}
$$
两个式子可统一为
$$
p(y \mid x; \theta) = (h_{\theta}(x))^{y} (1 - h_{\theta}(x))^{1 - y}.
$$
故似然函数为
$$
L(\theta) = \sum_{i = 1}^{n} (h(x^{(i)}))^{y^{(i)}} (1 - h(x^{(i)}))^{1 - y^{(i)}}.
$$
我们的目标是找出使 $L(\theta)$ 最大的 $\theta$，这等价于求使得其对数似然函数最大的 $\theta$：
$$
l(\theta) = \sum_{i = 1}^{n} y^{(i)} \log h(x^{(i)}) + (1 - y^{(i)}) \log (1 - h(x^{(i)})).
$$
接下来我们对 $\theta$ 求导。不妨先假设只有一个训练样本，此时其对 $\theta_j$ 的偏导为：
$$
\begin{align}
\frac{\partial}{\partial \theta_j} l(\theta) &= y \frac{\partial}{\partial \theta_j} \log g(\theta^{\text T} x) + (1 - y) \frac{\partial}{\partial \theta_j} \log (1 - g(\theta^{\text T} x)) \\
&= y \frac{1}{g(\theta^{\text T} x)} g(\theta^{\text T} x) (1 - g(\theta^{\text T} x)) x_j + (1 - y) \frac{1}{1 - g(\theta^{\text T} x)} \cdot (-1) \cdot g(\theta^{\text T} x) (1 - g(\theta^{\text T} x)) x_j \\
&= y (1 - g(\theta^{\text T} x)) x_j + (y - 1) g(\theta^{\text T} x) x_j \\
&= (y - g(\theta^{\text T} x)) x_j.
\end{align}
$$
上式第二个等号利用了链式求导法则及逻辑函数导数的特点。

由此，根据梯度上升（因为我们要最大化对数似然函数，应该顺着梯度的方向改变），我们可以得到随机逻辑回归：
$$
\theta_j := \theta_j + \alpha (y - h(x)) x_j.
$$
写成向量形式：
$$
\theta := \theta + \alpha (y - h(x)) x.
$$
可以发现，这和随机线性回归的公式在形式上是一模一样的！但事实上它们是两种完全不同的算法，因为 $h(x)$ 的取值是不同的。那么，形式上的统一性是否暗示着存在某种隐藏在这一形式背后的规律呢？我们将在 GLM 中作出回答。

## 另一种记号

下面我们给出另一种记号体系，它同样是非常有用的。

记 $l_{\text {logistic}}: \mathbb R \times \set{0, 1} \mapsto \mathbb R_{\ge 0}$ 为逻辑损失函数，它的定义为：
$$
l_{\text {logistic}} (t, y) \triangleq y \log (1 + \exp(-t)) + (1 - y) \log (1 + \exp(t)).
$$
易见它是对数似然函数的相反数：
$$
-l(\theta) = l_{\text {logistic}}(\theta^{\text T} x, y).
$$
有时 $\theta^{\text T} x$ 又被叫做逻辑特 (logit)。根据微积分知识，可有
$$
\frac{\partial}{\partial t} l_{\text {logistic}}(t, y) = g(t) - y = (1 + \exp(-t))^{-1} - y.
$$
由链式法则，可得
$$
\frac{\partial}{\partial \theta_j} l(\theta) = - \frac{\partial l_{\text{logistic}} (t, y)}{\partial t} \frac{\partial t}{\partial \theta_j} = (y - h(x))x_j.
$$
这与上面推导的结果相一致。
