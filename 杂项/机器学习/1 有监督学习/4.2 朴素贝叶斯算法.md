# 4.2 朴素贝叶斯算法

在 GDA 中，特征向量 $x$ 是连续随机变量。在本节中，我们介绍一种能应用于离散型随机变量的生成算法。

假设我们要做一个邮件分拣器，识别垃圾邮件与正常邮件。我们希望根据邮件的内容自动识别其是否为垃圾邮件。这其实是文本分类问题的一种。

假设我们有一个训练集，首先我们需要提取出数据的特征。我们可以构造一个特征向量，其长度与词典的长度相同，在第 $j$ 个位置上的数值表示邮件中是否存在字典中的第 $j$ 个单词，若存在，其值为 $1$；若不存在，其值为 $0$。例如，
$$
\begin{array}{c@{\hspace{1em}}c}
x = \begin{bmatrix}
1 \\
0 \\
0 \\
\vdots \\
1 \\
\vdots \\
0
\end{bmatrix} &
\begin{array}{c}
\text{a} \\
\text{aardvark} \\
\text{aardwolf} \\
\vdots \\
\text{buy} \\
\vdots \\
\text{zygmurgy}
\end{array}
\end{array}
$$
用来表示一个含有 a 和 buy 两词，但不包含 aardvark, aardwolf, zygmurgy 三词。我们将其称之为**词汇**。所以 $x$ 的维数等于词汇的大小。

现在，在构建了词汇向量的基础上，我们想要构造一个生成模型。所以，我们需要拟合 $p(x \mid y)$。但倘若我们的词汇有 50000 个词，$x$ 就有 $2^{50000}$ 种可能，如果我们要采用多元伯努利分布的方法，最终概率向量的维数就会变为 $2^{50000}$，这种计算开销是完全无法接受的！

为拟合 $p(x \mid y)$，我们需要做一个比较强的假设：假设 $x_i$ 关于 $y$ **条件独立**。这种假设被称为朴素贝叶斯假设，导出的算法叫做朴素贝叶斯分类器。例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件，然后在词汇中 "buy" 是第 $2087$ 个单词，而 "price" 是第 $39831$ 个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于 $x_{2087}$ （也就是单词 buy 是否出现在邮件里）的了解并不会影响你对 $x_{39831}$ （单词 price 出现的位置）的了解。将其写成数学表达式： $p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$。（要注意这并不是说 $x_{2087}$ 和 $x_{39831}$ 这两个特征是独立的，二者独立意味着 $p(x_{2087}) = p(x_{2087}|x_{39831})$，我们这里是说在给定了 $y$ 这样一个条件下，二者才是独立的，这就是条件独立的内涵）

由带有额外条件的条件概率公式，我们有
$$
\begin{align}
p(x_1, \cdots, x_{50000} \mid y) &= p(x_1 \mid y) p(x_2 \mid y, x_1) p(x_3 \mid y, x_1, x_2) \cdots p(x_{50000} \mid y, x_1, \cdots, x_{49999}) \\
&= p(x_1 \mid y) p(x_2 \mid y) \cdots p(x_{50000} \mid y) \\
&= \prod_{j = 1}^{d} p(x_j \mid y).
\end{align}
$$
我们的模型以 $\phi_{j \mid y = 1} = p(x_j = 1 \mid y = 1), \phi_{j \mid y = 0} = p(x_j= 1 \mid y = 0), \phi_y = p(y = 1)$ 为参数。像之前一样，我们给定一个训练集 $\set{(x^{(i)}, y^{(i)}) \mid i = 1, \cdots, n}$，我们可以写出下面的联合似然：
$$
L(\phi_y, \phi_{j \mid y = 0}, \phi_{j \mid y = 1}) = \prod_{i = 1}^{n} p(x^{(i)}, y^{(i)}).
$$
为使其最大化，需求各阶偏导数，经计算可有
$$
\begin{align}
\phi_{j \mid y = 1} &= \frac{\sum_{i = 1}^{n} I\set{x_j^{(i)} \wedge y^{(i)} = 1}}{\sum_{i = 1}^{n} I\set{y^{(i)} = 1}} \\
\phi_{j \mid y = 0} &= \frac{\sum_{i = 1}^{n} I\set{x_j^{(i)} \wedge y^{(i)} = 0}}{\sum_{i = 1}^{n} I\set{y^{(i)} = 0}} \\
\phi_y &= \frac{\sum_{i = 1}^{n} I\set{y^{(i)} = 1}}{n}
\end{align}
$$
对于上面三个式子，我们有着直观的解释：

- $\phi_{j \mid y = 1}$ 是在 $y = 1$ 的情况下出现特征 $x_j$ 的概率，分母是 $y = 1$ 的全体，即 $\sum_{i = 1}^{n} I \set{y^{(i)} = 1}$；分母是既满足 $y = 1$ 又满足 $x_j$ 的样本个数，即 $\sum_{i = 1}^{n} I \set{x_j^{(i)} \wedge y^{(i)} = 1}$。
- $\phi_{j \mid y = 0}$ 同理。
- $\phi_y$ 是先验概率，分母是全体样本量，分子是满足 $y = 1$ 的样本个数，为 $\sum_{i = 1}^{n} I \set{y^{(i)} = 1}$。

在得到这些参数后，我们带入贝叶斯公式，得到 $p(y = 1 \mid x)$：
$$
\begin{align}
p(y = 1 \mid x) &= \frac{p(x \mid y = 1) p(y = 1)}{p(x \mid y = 1) p(y = 1) + p(x \mid y = 0) p(y = 0)} \\
&= \frac{(\prod_{j = 1}^{d} p(x_j \mid y = 1)) p (y = 1)}{(\prod_{j = 1}^{d} p(x_j \mid y = 1)) p(y = 1) + (\prod_{j = 1}^{d} p(x_j \mid y = 0)) p(y = 0)}.
\end{align}
$$
代入计算即可，只需比较其与 $0.5$ 的大小便可做出决策。

最后我们要注意，刚刚我们对朴素贝叶斯算法的使用中，特征向量 $x_i$ 都是二项的，其实特征向量也可以是多个离散值，比如$\{1, 2, ..., k_i\}$ 这样也都是可以的。这时候只需要把对 $p(x_i|y)$ 的建模从伯努利分布改成多项式分布。实际上，即便一些原始的输入值是连续值（比如我们第一个案例中的房屋面积），也可以转换成一个小规模的离散值的集合，然后再使用朴素贝叶斯方法。例如，如果我们用特征向量 $x_i$ 来表示住房面积，那么就可以按照下面所示的方法来对这一变量进行离散化：

|   居住面积   | $<400$ | $400-800$ | $800-1200$ | $1200-1600$ | $>1600$ |
| :----------: | :----: | :-------: | :--------: | :---------: | :-----: |
| 离散值 $x_i$ |  $1$   |    $2$    |    $3$     |     $4$     |   $5$   |

这样，对于一个面积为 $890$ 平方英尺的房屋，就可以根据上面这个集合中对应的值来把特征向量的这一项的 $x_i$ 值设置为 $3$。然后就可以用朴素贝叶斯算法，并且将 $p(x_i|y)$ 作为多项式分布来进行建模，就都跟前面讲过的内容一样了。当原生的连续值的属性不太容易用一个多元正态分布来进行建模的时候，将其特征向量离散化然后使用朴素贝叶斯法（NB）来替代高斯判别分析法（GDA），通常能形成一个更好的分类器。