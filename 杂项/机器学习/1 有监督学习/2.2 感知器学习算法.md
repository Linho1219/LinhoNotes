# 2.2 感知器学习算法

考虑修改逻辑回归的逻辑函数，使其固定输出值为 $\set{0, 1}$. 一种实现方式是
$$
g(z) = \begin{cases}
1 & \text {if } z \ge 0 \\
0 & \text {if } z < 0 \\
\end{cases}.
$$
然后我们设 $h_{\theta}(x) = g(\theta^{\text T} x)$，这和逻辑回归的过程相同，只不过是把 $g$ 换成更新后的 $g$ 罢了。然后我们使用更新法则：
$$
\theta_j := \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}.
$$
于是我们就得到了感知器学习算法。

在 1960 年代，这个“感知器（perceptron）”被认为是对大脑中单个神经元工作方法的一个粗略建模。鉴于这个算法的简单程度，这个算法也是我们后续在本课程中讲学习理论的时候的起点。但一定要注意，虽然这个感知器学习算法可能看上去表面上跟我们之前讲的其他算法挺相似，但实际上这是一个和逻辑回归以及最小二乘线性回归等算法在种类上都完全不同的算法；尤其重要的是，很难对感知器的预测赋予有意义的概率解释，也很难作为一种最大似然估计算法来推出感知器学习算法。