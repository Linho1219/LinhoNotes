# 2.4 逻辑回归

回到逻辑回归。在逻辑回归中，我们需要最大化对数似然函数 $l(\theta)$。之前我们采用的是梯度上升法。接下来介绍另一种方法：牛顿迭代法。

牛顿迭代法的原理在高中学过，不再赘述。它可以帮助我们找到函数的零点。现在我们的目标是找到函数 $l(\theta)$ 的极大值点，这对应着它的导函数 $l'(\theta)$ 的零点，所以算法如下：
$$
\theta := \theta - \frac{l'(\theta)}{l''(\theta)}.
$$
由于在逻辑回归中 $\theta$ 是向量，我们需要将上式拓展到向量形式：
$$
\theta := \theta - H^{-1} \nabla_{\theta} l(\theta).
$$
其中 $H$ 是 Hessian 阵，第 $i$ 行 $j$ 列的表达式如下：
$$
H_{ij} = \frac{\partial^2 l(\theta)}{\partial \theta_i \partial \theta_j}.
$$
牛顿迭代法的收敛速度明显快于梯度法，但每一步迭代都需要更多的计算开销（计算 Hessian 阵及其逆矩阵）。但如果输入特征的维数 $d$ 不太大的话，总体而言牛顿迭代法还是有着极大的运算优势的。这里我们运用牛顿迭代法最大化对数似然函数的方法，又被称作费希尔评分法 (Fisher scoring)。